from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
import httpx
import os
from dotenv import load_dotenv
import time
from fastapi.middleware.cors import CORSMiddleware
import logging

# Configurer le logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv()

app = FastAPI()

# Ajouter CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Ou sp√©cifier les domaines exacts
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

HF_API_TOKEN = os.getenv("HF_API_TOKEN")
if not HF_API_TOKEN:
    logger.error("üö® HF_API_TOKEN manquant dans les variables d'environnement")
    # On continue l'ex√©cution, mais on avertit 

# üîç Liste des intentions possibles
INTENT_LABELS = ["recherche_web", "discussion", "generation_image", "generation_code", "autre"]

# Mod√®le de secours si HuggingFace √©choue
DEFAULT_INTENT_RULES = {
    "code": "generation_code",
    "programme": "generation_code",
    "script": "generation_code",
    "function": "generation_code",
    "cherche": "recherche_web",
    "trouve": "recherche_web",
    "recherche": "recherche_web",
    "quand": "recherche_web",
    "qui est": "recherche_web",
    "qu'est-ce que": "recherche_web",
    "dessine": "generation_image",
    "image": "generation_image",
    "photo": "generation_image",
}

class QuestionRequest(BaseModel):
    question: str

class IntentResult(BaseModel):
    intent: str
    confidence: float = 0.0
    
# Cache simple pour √©viter d'appeler HF trop souvent
intent_cache = {}
MAX_CACHE_SIZE = 1000

async def call_huggingface_model(question: str) -> tuple[str, float]:
    """Appelle le mod√®le Hugging Face pour classer l'intention"""
    # V√©rifier si l'intention est d√©j√† en cache
    if question in intent_cache:
        cached_intent, confidence = intent_cache[question]
        logger.info(f"üîÑ Utilisation du cache pour: '{question[:30]}...' -> {cached_intent}")
        return cached_intent, confidence
    
    # Si pas de token, utilisez le fallback
    if not HF_API_TOKEN:
        logger.warning("‚ö†Ô∏è HF_API_TOKEN non trouv√©, utilisation des r√®gles par d√©faut")
        return fallback_intent_detection(question)
    
    start_time = time.time()
    logger.info(f"üì§ Envoi √† HuggingFace: '{question[:50]}...'")
    
    headers = {
        "Authorization": f"Bearer {HF_API_TOKEN}"
    }

    payload = {
        "inputs": question,
        "parameters": {
            "candidate_labels": INTENT_LABELS,
            "multi_label": False
        }
    }

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
                headers=headers,
                json=payload,
                timeout=10.0  # Timeout de 10 secondes
            )

            elapsed = time.time() - start_time
            logger.info(f"‚è±Ô∏è R√©ponse HuggingFace re√ßue en {elapsed:.2f}s")
            
            if response.status_code != 200:
                logger.error(f"üö® Erreur HuggingFace: {response.status_code} {response.text[:100]}")
                return fallback_intent_detection(question)
                
            data = response.json()
            if "labels" in data and "scores" in data:
                intent = data["labels"][0]
                confidence = data["scores"][0]
                
                # Mettre en cache
                if len(intent_cache) >= MAX_CACHE_SIZE:
                    # Supprimer une entr√©e al√©atoire si le cache est plein
                    intent_cache.pop(next(iter(intent_cache)))
                intent_cache[question] = (intent, confidence)
                
                logger.info(f"‚úÖ Intention d√©tect√©e: {intent} (confiance: {confidence:.2f})")
                return intent, confidence
            else:
                logger.warning(f"‚ö†Ô∏è R√©ponse HF inattendue: {data}")
                return fallback_intent_detection(question)
    except Exception as e:
        logger.error(f"üö® Exception lors de l'appel HF: {str(e)}")
        return fallback_intent_detection(question)

def fallback_intent_detection(question: str) -> tuple[str, float]:
    """M√©thode de secours pour d√©tecter l'intention si HuggingFace √©choue"""
    question_lower = question.lower()
    
    # Recherche de mots-cl√©s dans la question
    for keyword, intent in DEFAULT_INTENT_RULES.items():
        if keyword in question_lower:
            logger.info(f"üîç Intention d√©tect√©e par r√®gles: {intent} (mot-cl√©: {keyword})")
            return intent, 0.7  # Confiance arbitraire
    
    # Par d√©faut, on consid√®re que c'est une discussion
    logger.info("üîÑ Aucun mot-cl√© trouv√©, utilisation de l'intention par d√©faut: discussion")
    return "discussion", 0.5

@app.post("/analyze", response_model=IntentResult)
async def analyze_question(data: QuestionRequest, request: Request):
    start_time = time.time()
    client_ip = request.client.host if request.client else "unknown"
    
    logger.info(f"üì© Question re√ßue de {client_ip}: '{data.question[:50]}...'")

    # Ajouter un d√©lai artificiel en cas d'appels trop rapides
    if hasattr(app, "last_request_time"):
        time_since_last = time.time() - app.last_request_time
        if time_since_last < 0.1:  # Si moins de 100ms depuis la derni√®re requ√™te
            await asyncio.sleep(0.2)  # Attendre 200ms

    app.last_request_time = time.time()

    try:
        intent, confidence = await call_huggingface_model(data.question)
        
        elapsed = time.time() - start_time
        logger.info(f"‚úÖ Traitement termin√© en {elapsed:.2f}s: {intent} ({confidence:.2f})")
        
        return {
            "intent": intent,
            "confidence": confidence
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'analyse: {str(e)}")
        # En cas d'erreur, retourner une intention par d√©faut
        return {
            "intent": "discussion",
            "confidence": 0.5
        }

@app.get("/health")
async def health_check():
    """Endpoint de v√©rification de sant√©"""
    return {"status": "ok", "service": "root-nlp-service"}

if __name__ == "__main__":
    import uvicorn
    import asyncio
    
    app.last_request_time = time.time()
    
    port = int(os.getenv("PORT", 8000))
    logger.info(f"üöÄ D√©marrage du service NLP sur le port {port}")
    
    uvicorn.run(app, host="0.0.0.0", port=port)