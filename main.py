from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
import httpx
import os
from dotenv import load_dotenv
import time
from fastapi.middleware.cors import CORSMiddleware
import logging
import asyncio
import re

# Configurer le logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv()

app = FastAPI()

# Ajouter CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

HF_API_TOKEN = os.getenv("HF_API_TOKEN")
if not HF_API_TOKEN:
    logger.warning("üö® HF_API_TOKEN manquant dans les variables d'environnement")

# üîç Liste des intentions possibles
INTENT_LABELS = [
    "recherche_web",           # chercher une info
    "discussion",            # discuter, parler
    "generation_image",        # cr√©er une image
    "generation_code",         # g√©n√©rer du code
    "generation_texte",        # r√©diger, inventer
    "analyse_donnee",          # comprendre ou synth√©tiser des infos
    "planification",           # demander de l'organisation
    "conseil_emotionnel",      # besoin de soutien ou de motivation
    "question_personnelle",    # introspection ou autoanalyse
    "autre"                    # tout ce qui ne rentre dans rien
]


# R√®gles de secours au cas o√π Hugging Face √©choue
FALLBACK_RULES = {
    # Mots-cl√©s qui indiquent fortement une intention
    "keywords": {
        "generation_code": ["code", "programme", "script", "fonction", "programmer", "d√©velopper"],
        "recherche_web": ["cherche", "m√©t√©o", "pr√©sident", "capitale", "d√©finition"],
        "generation_image": ["dessine", "image", "visualise", "dessin"]
    },
    
    # Patterns qui indiquent fortement une intention
    "patterns": {
        "recherche_web": [
            r"(?i).*m√©t√©o.*",
            r"(?i).*quel temps.*√†.*",
            r"(?i).*qui est le pr√©sident.*",
            r"(?i).*quelle est la capitale.*",
            r"(?i).*o√π se trouve.*",
            r"(?i).*combien.*co√ªte.*",
            r"(?i).*parapluie.*demain.*",
            r"(?i).*faut[- ]il.*parapluie.*",
            r"(?i).*vais[- ]je.*prendre.*parapluie.*",
            r"(?i).*pleuvoir.*demain.*",
            r"(?i).*pluie.*demain.*",
            

        ],
        "generation_image": [
            r"(?i)dessine[- ]moi.*",
            r"(?i)g√©n√®re[- ]moi une image.*",
        ],
        "generation_code": [
            r"(?i)√©cris[- ]moi un (code|programme|script).*",
            r"(?i)comment coder.*",
        ]
    }
}

class QuestionRequest(BaseModel):
    question: str

class IntentResult(BaseModel):
    intent: str
    confidence: float = 0.0
    
# Cache simple pour √©viter d'appeler HF trop souvent
intent_cache = {}
MAX_CACHE_SIZE = 1000

async def call_huggingface_model(question: str) -> tuple[str, float]:
    """Appelle le mod√®le Hugging Face pour classer l'intention"""
    # V√©rifier si l'intention est d√©j√† en cache
    if question in intent_cache:
        cached_intent, confidence = intent_cache[question]
        logger.info(f"üîÑ Utilisation du cache pour: '{question[:30]}...' -> {cached_intent}")
        return cached_intent, confidence
    
    # Si pas de token, utiliser le fallback
    if not HF_API_TOKEN:
        logger.warning("‚ö†Ô∏è HF_API_TOKEN non trouv√©, utilisation des r√®gles par d√©faut")
        return fallback_intent_detection(question)
    
    start_time = time.time()
    logger.info(f"üì§ Envoi √† HuggingFace: '{question[:50]}...'")
    
    headers = {
        "Authorization": f"Bearer {HF_API_TOKEN}"
    }

    # Am√©lioration du prompt pour mieux diriger le mod√®le
    payload = {
        "inputs": question,
        "parameters": {
            "candidate_labels": INTENT_LABELS,
            "multi_label": False,
            "hypothesis_template": "Dans le contexte de cette requ√™te, l'utilisateur souhaite principalement obtenir un r√©sultat relevant de la cat√©gorie suivante : {}."
        }
    }

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
                headers=headers,
                json=payload,
                timeout=10.0  # Timeout de 10 secondes
            )

            elapsed = time.time() - start_time
            logger.info(f"‚è±Ô∏è R√©ponse HuggingFace re√ßue en {elapsed:.2f}s")
            
            if response.status_code != 200:
                logger.error(f"üö® Erreur HuggingFace: {response.status_code} {response.text[:100]}")
                return fallback_intent_detection(question)
                
            data = response.json()
            if "labels" in data and "scores" in data:
                intent = data["labels"][0]
                confidence = data["scores"][0]
                
                # Si la confiance est tr√®s faible, on v√©rifie avec les r√®gles
                if confidence < 0.6:
                    logger.warning(f"‚ö†Ô∏è Confiance faible ({confidence:.2f}), v√©rification avec r√®gles")
                    fallback_intent, fallback_confidence = fallback_intent_detection(question)
                    
                    # Si les r√®gles ont une confiance plus √©lev√©e, on les utilise
                    if fallback_confidence > confidence + 0.1:  # +0.1 pour favoriser HF quand c'est proche
                        logger.info(f"üîÑ Utilisation du fallback: {fallback_intent} (confiance: {fallback_confidence:.2f})")
                        intent = fallback_intent
                        confidence = fallback_confidence
                
                # Mettre en cache
                if len(intent_cache) >= MAX_CACHE_SIZE:
                    # Supprimer une entr√©e al√©atoire si le cache est plein
                    intent_cache.pop(next(iter(intent_cache)))
                intent_cache[question] = (intent, confidence)
                
                logger.info(f"‚úÖ Intention d√©tect√©e: {intent} (confiance: {confidence:.2f})")
                return intent, confidence
            else:
                logger.warning(f"‚ö†Ô∏è R√©ponse HF inattendue: {data}")
                return fallback_intent_detection(question)
    except Exception as e:
        logger.error(f"üö® Exception lors de l'appel HF: {str(e)}")
        return fallback_intent_detection(question)

def fallback_intent_detection(question: str) -> tuple[str, float]:
    """M√©thode de secours pour d√©tecter l'intention si HuggingFace √©choue"""
    question_lower = question.lower()
    
    # V√©rifier d'abord les patterns
    for intent, patterns in FALLBACK_RULES["patterns"].items():
        for pattern in patterns:
            if re.match(pattern, question):
                logger.info(f"üîç Pattern d√©tect√© pour {intent}: {pattern}")
                return intent, 0.85
    
    # Ensuite v√©rifier les mots-cl√©s
    for intent, keywords in FALLBACK_RULES["keywords"].items():
        for keyword in keywords:
            if keyword.lower() in question_lower:
                logger.info(f"üîë Mot-cl√© d√©tect√© pour {intent}: {keyword}")
                return intent, 0.8
    
    # Analyse des mots interrogatifs pour la recherche web
    if question.strip().endswith("?") and any(question_lower.startswith(w) for w in 
                                             ["qui", "que", "quoi", "quel", "quelle", 
                                              "quels", "quelles", "o√π", "comment", 
                                              "pourquoi", "quand", "combien"]):
        logger.info("‚ùì Question d√©tect√©e, suggestion de 'recherche_web'")
        return "recherche_web", 0.7
    
    # Par d√©faut, on consid√®re que c'est une discussion
    logger.info("üí¨ Aucun pattern sp√©cifique trouv√©, utilisation de l'intention par d√©faut 'discussion'")
    return "discussion", 0.6

@app.post("/analyze", response_model=IntentResult)
async def analyze_question(data: QuestionRequest, request: Request):
    start_time = time.time()
    client_ip = request.client.host if request.client else "unknown"
    
    logger.info(f"üì© Question re√ßue de {client_ip}: '{data.question[:50]}...'")

    # Ajouter un d√©lai artificiel en cas d'appels trop rapides
    if hasattr(app, "last_request_time"):
        time_since_last = time.time() - app.last_request_time
        if time_since_last < 0.1:  # Si moins de 100ms depuis la derni√®re requ√™te
            await asyncio.sleep(0.2)  # Attendre 200ms

    app.last_request_time = time.time()

    try:
        intent, confidence = await call_huggingface_model(data.question)
        
        elapsed = time.time() - start_time
        logger.info(f"‚úÖ Traitement termin√© en {elapsed:.2f}s: {intent} ({confidence:.2f})")
        
        return {
            "intent": intent,
            "confidence": confidence
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'analyse: {str(e)}")
        # En cas d'erreur, retourner une intention par d√©faut
        return {
            "intent": "discussion",
            "confidence": 0.5
        }

@app.get("/health")
async def health_check():
    """Endpoint de v√©rification de sant√©"""
    return {"status": "ok", "service": "root-nlp-service"}

if __name__ == "__main__":
    import uvicorn
    
    app.last_request_time = time.time()
    
    port = int(os.getenv("PORT", 8000))
    logger.info(f"üöÄ D√©marrage du service NLP sur le port {port}")
    
    uvicorn.run(app, host="0.0.0.0", port=port)